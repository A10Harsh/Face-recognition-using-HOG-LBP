{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e091a558",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<>:5: SyntaxWarning: invalid escape sequence '\\c'\n",
      "<>:5: SyntaxWarning: invalid escape sequence '\\c'\n",
      "C:\\Users\\Harsh\\AppData\\Local\\Temp\\ipykernel_23428\\783434265.py:5: SyntaxWarning: invalid escape sequence '\\c'\n",
      "  model = load_model(\"cnn model\\cnnFace.keras\")  # or .keras\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1s/step\n",
      "✅ Embedding model created successfully!\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 343ms/step\n",
      "Output shape: (1, 128)\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.models import load_model, Sequential\n",
    "import numpy as np\n",
    "\n",
    "# ✅ Load your model\n",
    "model = load_model(\"cnn model\\cnnFace.keras\")  # or .keras\n",
    "\n",
    "# ✅ Force model to build by calling it once\n",
    "dummy_input = np.zeros((1, 128, 128, 1), dtype=np.float32)  # adjust size if different\n",
    "_ = model.predict(dummy_input)\n",
    "\n",
    "# ✅ Now safely remove the final layer\n",
    "embedding_model = Sequential(model.layers[:-1])\n",
    "\n",
    "# ✅ Test the embedding model\n",
    "print(\"✅ Embedding model created successfully!\")\n",
    "print(\"Output shape:\", embedding_model.predict(dummy_input).shape)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38e104ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "from skimage.feature import local_binary_pattern\n",
    "\n",
    "# ✅ CLAHE Function\n",
    "def apply_CLAHE(image):\n",
    "    clahe = cv2.createCLAHE(clipLimit=2.0, tileGridSize=(8, 8))\n",
    "    return clahe.apply(image)\n",
    "\n",
    "# ✅ Retinex Function (Simple Single-Scale)\n",
    "def apply_retinex(image):\n",
    "    image = image.astype(np.float32) + 1.0  # Prevent log(0)\n",
    "    retinex = np.log(image) - np.log(cv2.GaussianBlur(image, (0, 0), sigmaX=30))\n",
    "    retinex = cv2.normalize(retinex, None, alpha=0, beta=255, norm_type=cv2.NORM_MINMAX)\n",
    "    return retinex.astype(np.uint8)\n",
    "\n",
    "# ✅ Uniform LBP Function\n",
    "def apply_uniform_lbp(image, P=8, R=1):\n",
    "    image = cv2.imread(image_path, cv2.IMREAD_GRAYSCALE)\n",
    "    lbp = local_binary_pattern(image, P, R, method='uniform')\n",
    "    (hist, _) = np.histogram(lbp.ravel(),\n",
    "                             bins=np.arange(0, P + 3),\n",
    "                             range=(0, P + 2))\n",
    "    hist = hist.astype(\"float\")\n",
    "    hist /= (hist.sum() + 1e-7)  # Normalize\n",
    "    return hist\n",
    "\n",
    "# ✅ Final Embedding Function\n",
    "def get_embedding(img_path):\n",
    "    img = cv2.imread(img_path, cv2.IMREAD_GRAYSCALE)\n",
    "    if img is None:\n",
    "        raise ValueError(f\"Image not found: {img_path}\")\n",
    "\n",
    "    # Step 1: Resize (optional first resize if images are inconsistent)\n",
    "    img = cv2.resize(img, (128, 128))\n",
    "\n",
    "    # Step 2: Apply CLAHE\n",
    "    img = apply_CLAHE(img)\n",
    "\n",
    "    # Step 3: Apply Retinex\n",
    "    img = apply_retinex(img)\n",
    "\n",
    "    # Step 4: Apply Uniform LBP\n",
    "    img = apply_uniform_lbp(img)\n",
    "    print(img)\n",
    "\n",
    "\n",
    "  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0160695d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 104ms/step\n",
      "✅ Embedding generated successfully!\n",
      "Embedding shape: (128,)\n",
      "Embedding sample values: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n"
     ]
    }
   ],
   "source": [
    "# ✅ Test get_embedding() on one image\n",
    "test_img_path = \"img.jpg\"  # update to an existing file\n",
    "\n",
    "try:\n",
    "    emb = get_embedding(test_img_path)\n",
    "    print(\"✅ Embedding generated successfully!\")\n",
    "    print(\"Embedding shape:\", emb.shape)\n",
    "    print(\"Embedding sample values:\", emb[:10])\n",
    "except Exception as e:\n",
    "    print(\"❌ Error:\", e)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "28cdf3e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_model = Sequential(model.layers[:-1])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "25796fcb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 conv2d ⚠️ No output shape (maybe not built yet)\n",
      "1 batch_normalization ⚠️ No output shape (maybe not built yet)\n",
      "2 max_pooling2d ⚠️ No output shape (maybe not built yet)\n",
      "3 conv2d_1 ⚠️ No output shape (maybe not built yet)\n",
      "4 batch_normalization_1 ⚠️ No output shape (maybe not built yet)\n",
      "5 max_pooling2d_1 ⚠️ No output shape (maybe not built yet)\n",
      "6 conv2d_2 ⚠️ No output shape (maybe not built yet)\n",
      "7 batch_normalization_2 ⚠️ No output shape (maybe not built yet)\n",
      "8 max_pooling2d_2 ⚠️ No output shape (maybe not built yet)\n",
      "9 flatten ⚠️ No output shape (maybe not built yet)\n",
      "10 dense ⚠️ No output shape (maybe not built yet)\n",
      "11 dropout ⚠️ No output shape (maybe not built yet)\n",
      "12 dense_1 ⚠️ No output shape (maybe not built yet)\n"
     ]
    }
   ],
   "source": [
    "for idx, layer in enumerate(model.layers):\n",
    "    try:\n",
    "        print(idx, layer.name, layer.output_shape)\n",
    "    except:\n",
    "        print(idx, layer.name, \"⚠️ No output shape (maybe not built yet)\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b7a06ffb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:5 out of the last 6 calls to <function TensorFlowTrainer.make_predict_function.<locals>.one_step_on_data_distributed at 0x000002186F7B7600> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 633ms/step\n",
      "✅ Embedding model created using layer slicing!\n",
      "WARNING:tensorflow:6 out of the last 7 calls to <function TensorFlowTrainer.make_predict_function.<locals>.one_step_on_data_distributed at 0x000002186F7B5260> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 275ms/step\n",
      "Output shape: (1, 128)\n",
      "Sample values: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.models import load_model, Sequential\n",
    "import numpy as np\n",
    "\n",
    "# ✅ 1. Load the original model\n",
    "model = load_model(\"cnn model/cnnFace.keras\")  # Update path if needed\n",
    "\n",
    "# ✅ 2. Force the model to build\n",
    "dummy_input = np.zeros((1, 128, 128, 1), dtype=np.float32)\n",
    "_ = model.predict(dummy_input)  # Builds the model graph\n",
    "\n",
    "# ✅ 3. Create embedding model by removing last TWO layers\n",
    "# Final layers:\n",
    "# 10 = Dense\n",
    "# 11 = Dropout\n",
    "# 12 = Dense (classifier)\n",
    "embedding_model = Sequential(model.layers[:11])  # keep up to index 10\n",
    "\n",
    "# ✅ 4. Test output\n",
    "print(\"✅ Embedding model created using layer slicing!\")\n",
    "test_emb = embedding_model.predict(dummy_input)\n",
    "print(\"Output shape:\", test_emb.shape)\n",
    "print(\"Sample values:\", test_emb[0][:10])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89c70c6a",
   "metadata": {},
   "source": [
    "directly processing the classfication cnn "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "0eedf76f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import load_model\n",
    "model = load_model(\"cnn model/cnnFace.keras\")  # update path if needed\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5671770",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "from skimage.feature import local_binary_pattern\n",
    "\n",
    "# ✅ CLAHE\n",
    "def apply_CLAHE(image):\n",
    "    clahe = cv2.createCLAHE(clipLimit=2.0, tileGridSize=(8, 8))\n",
    "    return clahe.apply(image)\n",
    "\n",
    "# ✅ Retinex\n",
    "def apply_retinex(image):\n",
    "    image = image.astype(np.float32) + 1.0\n",
    "    retinex = np.log(image) - np.log(cv2.GaussianBlur(image, (0, 0), sigmaX=30))\n",
    "    retinex = cv2.normalize(retinex, None, 0, 255, cv2.NORM_MINMAX)\n",
    "    return retinex.astype(np.uint8)\n",
    "\n",
    "# ✅ LBP\n",
    "def apply_uniform_lbp(image, P=8, R=1):\n",
    "    lbp = local_binary_pattern(image, P, R, method=\"uniform\")\n",
    "    lbp = (lbp / lbp.max()) * 255\n",
    "    return lbp.astype(np.uint8)\n",
    "\n",
    "# ✅ Predict function\n",
    "def predict_image(img_path):\n",
    "    img = cv2.imread(img_path, cv2.IMREAD_GRAYSCALE)\n",
    "    img = cv2.resize(img, (128, 128))\n",
    "\n",
    "    img = apply_CLAHE(img)\n",
    "    img = apply_retinex(img)\n",
    "    img = apply_uniform_lbp(img)\n",
    "\n",
    "    img = img / 255.0\n",
    "    img = img.reshape(1, 128, 128, 1)\n",
    "\n",
    "    prediction = model.predict(img)\n",
    "    class_id = np.argmax(prediction)\n",
    "    confidence = np.max(prediction)\n",
    "\n",
    "    return class_id, confidence\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "56a20fa1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Resuming download from 991952896 bytes (1502058198 bytes left)...\n",
      "Resuming download from https://www.kaggle.com/api/v1/datasets/download/hearfool/vggface2?dataset_version_number=1 (991952896/2494011094) bytes left.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2.32G/2.32G [08:08<00:00, 3.07MB/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting files...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Path to dataset files: C:\\Users\\Harsh\\.cache\\kagglehub\\datasets\\hearfool\\vggface2\\versions\\1\n"
     ]
    }
   ],
   "source": [
    "import kagglehub\n",
    "\n",
    "# Download latest version\n",
    "path = kagglehub.dataset_download(\"hearfool/vggface2\")\n",
    "\n",
    "print(\"Path to dataset files:\", path)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
